{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99f4a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded .env from: d:\\Research Assistant Bot\\Research-Assistant-Bot\\.env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import operator\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- LIBRARIES ---\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, get_buffer_string\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import Send \n",
    "\n",
    "# 1. Load Environment Variables\n",
    "found_dotenv = find_dotenv()\n",
    "if found_dotenv:\n",
    "    load_dotenv(found_dotenv)\n",
    "    print(f\"‚úÖ Loaded .env from: {found_dotenv}\")\n",
    "else:\n",
    "    print(\"‚ùå Warning: .env file not found.\")\n",
    "\n",
    "if not os.getenv(\"GROQ_API_KEY\") or not os.getenv(\"TAVILY_API_KEY\"):\n",
    "    raise ValueError(\"API Keys are missing. Check your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce054c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models initialized with Auto-Retry protection.\n"
     ]
    }
   ],
   "source": [
    "# 2. Initialize Models (WITH RATE LIMIT PROTECTION)\n",
    "\n",
    "# The Planner: Smart, Structured (Llama 3.3 70B)\n",
    "llm_planner = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# The Worker: Fast, High Rate Limit (Llama 4 17B)\n",
    "# FIX 1: Add .with_retry() to automatically handle 429 errors\n",
    "llm_worker = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0\n",
    ").with_retry(\n",
    "    stop_after_attempt=8,         # Retry up to 8 times\n",
    "    wait_exponential_jitter=True  # Randomly increase wait time (2s, 4s, 8s...)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Models initialized with Auto-Retry protection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9336fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA MODELS ---\n",
    "class Analyst(BaseModel):\n",
    "    affiliation: str = Field(description=\"Primary affiliation of the analyst.\")\n",
    "    name: str = Field(description=\"Name of the analyst.\")\n",
    "    role: str = Field(description=\"Role of the analyst in the context of the topic.\")\n",
    "    description: str = Field(description=\"Description of the analyst focus, concerns, and motives.\")\n",
    "    \n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    analysts: List[Analyst] = Field(description=\"Comprehensive list of analysts.\")\n",
    "\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    topic: str\n",
    "    max_analysts: int\n",
    "    human_analyst_feedback: str\n",
    "    analysts: List[Analyst]\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    max_num_turns: int\n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Analyst\n",
    "    interview: str\n",
    "    sections: list\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    topic: str \n",
    "    max_analysts: int \n",
    "    human_analyst_feedback: str \n",
    "    analysts: List[Analyst] \n",
    "    sections: Annotated[list, operator.add] \n",
    "    introduction: str \n",
    "    content: str \n",
    "    conclusion: str \n",
    "    final_report: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2770005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prash\\AppData\\Local\\Temp\\ipykernel_22920\\276714087.py:2: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3) # Reduced to 2 to save tokens\n"
     ]
    }
   ],
   "source": [
    "# --- TOOLS & PROMPTS ---\n",
    "tavily_search = TavilySearchResults(max_results=3) # Reduced to 2 to save tokens\n",
    "\n",
    "analyst_instructions = \"\"\"You are tasked with creating a set of AI analyst personas. \n",
    "Review the topic: {topic}\n",
    "Review feedback: {human_analyst_feedback}\n",
    "Pick the top {max_analysts} themes and assign one analyst to each.\"\"\"\n",
    "\n",
    "question_instructions = \"\"\"You are an analyst interviewing an expert. \n",
    "Your goal is to gather interesting and specific insights.\n",
    "Topic & Goals: {goals}\n",
    "Introduce yourself, ask questions, and say 'Thank you so much for your help!' when done.\"\"\"\n",
    "\n",
    "answer_instructions = \"\"\"You are an expert answering an analyst.\n",
    "Use ONLY this context: {context}\n",
    "Cite sources like [1] next to statements. List sources at the bottom.\"\"\"\n",
    "\n",
    "section_writer_instructions = \"\"\"You are an expert technical writer. \n",
    "Your task is to create a section of a report based *strictly* on the provided source documents.\n",
    "\n",
    "Target Audience: C-Level Executives and Technical Leads.\n",
    "Tone: Professional, data-driven, and concise.\n",
    "\n",
    "Instructions:\n",
    "1. **Analyze:** Read the provided context. Identify key statistics, dates, quotes, and technical specifications.\n",
    "2. **Draft:** Write a summary of the provided context.\n",
    "   - Use strictly factual language.\n",
    "   - If there are conflicting facts in the sources, mention the conflict.\n",
    "   - Do NOT use phrases like \"The text says\" or \"According to the document.\" Just state the facts.\n",
    "3. **Structure:**\n",
    "   - Start with a strong opening sentence summarizing the main insight.\n",
    "   - Use bullet points for lists or features.\n",
    "   - Limit length to ~300 words.\n",
    "\n",
    "Title: {focus}\"\"\"\n",
    "\n",
    "report_writer_instructions = \"\"\"You are a Lead Research Editor compiling a final report on: {topic}\n",
    "\n",
    "You have received memos from a team of analysts. Each memo contains specific insights and a list of \"Raw Sources\".\n",
    "\n",
    "**Your Goal:**\n",
    "Write a cohesive, professional \"State of the Union\" style report. Do NOT just copy-paste the memos one by one. Instead, synthesize the information into a unified narrative.\n",
    "\n",
    "**Strict Requirements:**\n",
    "1. **Thematic Organization:** Group related insights from different analysts together. If Analyst A and Analyst B both mentioned \"Cost,\" combine those insights into a single \"Financial Implications\" section.\n",
    "2. **Conflict Resolution:** If analysts provide conflicting data, present the range (e.g., \"Estimates range from X to Y\").\n",
    "3. **Citation Handling:** - You MUST use the \"Raw Sources\" provided.\n",
    "   - Cite statements using [1], [2], etc.\n",
    "   - Ensure every claim is backed by a citation number.\n",
    "\n",
    "**Output Structure:**\n",
    "# {topic}\n",
    "\n",
    "## Executive Summary\n",
    "(A 3-sentence high-level overview of the findings)\n",
    "\n",
    "## Key Insights\n",
    "(The main body. Use subheaders like 'Market Trends', 'Technical Architecture', 'Risks', etc., based on the content. DO NOT use analyst names as headers.)\n",
    "\n",
    "## Sources\n",
    "(List the unique URLs provided in the memos, numbered [1], [2], etc.)\n",
    "\n",
    "Memos to process: \n",
    "{context}\"\"\"\n",
    "\n",
    "\n",
    "intro_conclusion_instructions = \"\"\"Write a crisp {topic} Introduction or Conclusion.\n",
    "Use headers: ## Introduction or ## Conclusion.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680aeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NODES (PLANNER 70B) ---\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    topic = state['topic']\n",
    "    max_analysts = state['max_analysts']\n",
    "    feedback = state.get('human_analyst_feedback', '')\n",
    "    structured_llm = llm_planner.with_structured_output(Perspectives)\n",
    "    system_msg = analyst_instructions.format(topic=topic, human_analyst_feedback=feedback, max_analysts=max_analysts)\n",
    "    analysts = structured_llm.invoke([SystemMessage(content=system_msg)]+[HumanMessage(content=\"Generate analysts.\")])\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "    formatted_sections = \"\\n\\n\".join([f\"{s}\" for s in sections])\n",
    "    system_msg = report_writer_instructions.format(topic=topic, context=formatted_sections)\n",
    "    report = llm_planner.invoke([SystemMessage(content=system_msg)]+[HumanMessage(content=\"Write report.\")])\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "    formatted_sections = \"\\n\\n\".join([f\"{s}\" for s in sections])\n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_sections)\n",
    "    intro = llm_planner.invoke([instructions]+[HumanMessage(content=\"Write introduction.\")])\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "    formatted_sections = \"\\n\\n\".join([f\"{s}\" for s in sections])\n",
    "    instructions = intro_conclusion_instructions.format(topic=topic, formatted_str_sections=formatted_sections)\n",
    "    conclusion = llm_planner.invoke([instructions]+[HumanMessage(content=\"Write conclusion.\")])\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "# --- NODES (WORKER 17B) ---\n",
    "def generate_question(state: InterviewState):\n",
    "    # FIX 2: Small random sleep to prevent \"thundering herd\"\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "    \n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    system_msg = question_instructions.format(goals=analyst.persona)\n",
    "    question = llm_worker.invoke([SystemMessage(content=system_msg)]+messages)\n",
    "    return {\"messages\": [question]}\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\" Search using Direct Prompting (More stable for 17B) \"\"\"\n",
    "    messages = state['messages']\n",
    "    prompt = SystemMessage(content=\"Generate a concise web search query to help answer the analyst's last question. Return ONLY the query string.\")\n",
    "    response = llm_worker.invoke(messages + [prompt])\n",
    "    search_query = response.content.strip('\"').strip()\n",
    "    \n",
    "    try:\n",
    "        results = tavily_search.invoke({\"query\": search_query})\n",
    "        data = results if isinstance(results, list) else [results]\n",
    "        formatted = \"\\n\\n---\\n\\n\".join(\n",
    "            [f'<Document href=\"{doc.get(\"url\", \"\")}\"/>\\n{doc.get(\"content\", \"\")}\\n</Document>' for doc in data]\n",
    "        )\n",
    "    except:\n",
    "        formatted = f\"Search failed for: {search_query}\"\n",
    "\n",
    "    return {\"context\": [formatted]}\n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    messages = state['messages']\n",
    "    prompt = SystemMessage(content=\"Generate a concise Wikipedia search term. Return ONLY the term.\")\n",
    "    response = llm_worker.invoke(messages + [prompt])\n",
    "    search_query = response.content.strip('\"').strip()\n",
    "    \n",
    "    try:\n",
    "        docs = WikipediaLoader(query=search_query, load_max_docs=3).load()\n",
    "        formatted = \"\\n\\n---\\n\\n\".join(\n",
    "            [f'<Document source=\"{d.metadata.get(\"source\", \"Wiki\")}\"/>\\n{d.page_content}\\n</Document>' for d in docs]\n",
    "        )\n",
    "    except:\n",
    "        formatted = \"No wikipedia results.\"\n",
    "    return {\"context\": [formatted]}\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "    system_msg = answer_instructions.format(goals=analyst.persona, context=context)\n",
    "    answer = llm_worker.invoke([SystemMessage(content=system_msg)]+messages)\n",
    "    answer.name = \"expert\"\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    return {\"interview\": get_buffer_string(state[\"messages\"])}\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    # FIX 3: Longer random sleep for the heavy \"Summary\" node\n",
    "    delay = random.uniform(5, 10)\n",
    "    print(f\"    (Summarizing... Staggering {delay:.1f}s)\")\n",
    "    time.sleep(delay)\n",
    "\n",
    "    analyst = state[\"analyst\"]\n",
    "    context = state[\"context\"]\n",
    "    \n",
    "    # Generate Summary\n",
    "    system_msg = section_writer_instructions.format(focus=analyst.description)\n",
    "    section = llm_worker.invoke([SystemMessage(content=system_msg)]+[HumanMessage(content=f\"Use this source: {context}\")])\n",
    "    \n",
    "    # FIX 4: Programmatically Extract URLs for High Quality Sources\n",
    "    urls = re.findall(r'href=\"(.*?)\"', str(context))\n",
    "    section_content = section.content\n",
    "    if urls:\n",
    "        section_content += \"\\n\\n### Raw Sources\\n\"\n",
    "        for url in set(urls):\n",
    "            section_content += f\"- {url}\\n\"\n",
    "            \n",
    "    return {\"sections\": [section_content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49419e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROUTING ---\n",
    "def human_feedback(state): pass\n",
    "\n",
    "def should_continue(state):\n",
    "    if state.get('human_analyst_feedback'): return \"create_analysts\"\n",
    "    return \"conduct_interview\"\n",
    "\n",
    "def route_messages(state, name=\"expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    if len([m for m in messages if isinstance(m, AIMessage) and m.name == name]) >= state.get('max_num_turns', 2):\n",
    "        return 'save_interview'\n",
    "    if \"Thank you so much for your help\" in messages[-2].content:\n",
    "        return 'save_interview'\n",
    "    return \"ask_question\"\n",
    "\n",
    "def initiate_all_interviews(state):\n",
    "    if state.get('human_analyst_feedback'): return \"create_analysts\"\n",
    "    topic = state[\"topic\"]\n",
    "    return [Send(\"conduct_interview\", {\n",
    "        \"analyst\": analyst,\n",
    "        \"messages\": [HumanMessage(content=f\"So you said you were writing an article on {topic}?\")]\n",
    "    }) for analyst in state[\"analysts\"]]\n",
    "\n",
    "def finalize_report(state):\n",
    "    content = state[\"content\"].replace(\"## Insights\", \"\").strip()\n",
    "    if \"## Sources\" in content:\n",
    "        content, sources = content.split(\"\\n## Sources\\n\")\n",
    "    else:\n",
    "        sources = None\n",
    "    final = f\"{state['introduction']}\\n\\n---\\n\\n## Insights\\n{content}\\n\\n---\\n\\n{state['conclusion']}\"\n",
    "    if sources: final += f\"\\n\\n## Sources\\n{sources}\"\n",
    "    return {\"final_report\": final}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4a45cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Graph compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- BUILD GRAPHS ---\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
    "interview_builder.add_conditional_edges(\"answer_question\", route_messages, ['ask_question', 'save_interview'])\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "interview_graph = interview_builder.compile()\n",
    "\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_graph)\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\"human_feedback\", initiate_all_interviews, [\"create_analysts\", \"conduct_interview\"])\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "print(\"üöÄ Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0adc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è Generating Analysts...\n",
      "‚úÖ Generated 3 Analysts.\n",
      "   - Dr. Rachel Kim (AI Ethics Specialist)\n",
      "   - Jack Taylor (Market Analyst)\n",
      "   - Professor Liam Chen (Security Expert)\n",
      "\n",
      "üöÄ Starting Parallel Research (This may take 2-3 minutes due to staggering)...\n",
      "    (Summarizing... Staggering 9.0s)\n",
      "    (Summarizing... Staggering 8.5s)\n",
      "    (Summarizing... Staggering 5.1s)\n",
      "-- Finished Node: conduct_interview\n",
      "-- Finished Node: conduct_interview\n",
      "-- Finished Node: conduct_interview\n",
      "-- Finished Node: write_introduction\n",
      "-- Finished Node: write_conclusion\n",
      "-- Finished Node: write_report\n",
      "-- Finished Node: finalize_report\n",
      "\n",
      "\n",
      "==============================\n",
      "FINAL REPORT\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Introduction\n",
       "As we approach 2025, the world is on the cusp of a revolution in artificial intelligence (AI). AI agents, once considered the stuff of science fiction, are now becoming an integral part of our daily lives. From virtual assistants to autonomous vehicles, AI agents are transforming the way we live, work, and interact with one another. With advancements in machine learning, natural language processing, and computer vision, AI agents are becoming increasingly sophisticated, enabling them to perform complex tasks with unprecedented accuracy and efficiency. As we look to the future, it is clear that AI agents will play a vital role in shaping the world of 2025 and beyond, bringing about unprecedented opportunities for innovation, growth, and transformation.\n",
       "\n",
       "---\n",
       "\n",
       "## Insights\n",
       "# The Future of AI Agents in 2025\n",
       "\n",
       "## Executive Summary\n",
       "The year 2025 is poised to be a pivotal year for the adoption of AI agents, with significant implications for various industries and aspects of society. As AI agents become increasingly autonomous, capable of controlling other agents, buying goods and services, negotiating with one another, and creating new agents, concerns about bias, job displacement, and the need for new cultural norms are growing [1]. The market potential of AI agents is substantial, driven by their ability to operate autonomously in complex environments, prioritize decision-making over content creation, and integrate with various software tools [2]. However, the increasing use of AI agents also introduces new security risks, with researchers identifying a wave of attacks targeting AI agents' features [3].\n",
       "\n",
       "## Key Insights\n",
       "\n",
       "### Market Trends\n",
       "The global AI agent market is projected to grow at a compound annual growth rate (CAGR) of over 35% between 2025 and 2030, reaching a valuation of approximately $45 billion by 2030 [4]. Estimates suggest that by 2026, 40% of enterprise apps will feature task-specific AI agents, up from less than 5% in 2025 [5]. The emergence of \"vibe coding,\" a trend of using generative AI to spin up code from plain-language prompts, has democratized development but introduced security and reliability concerns [6].\n",
       "\n",
       "### Technical Architecture\n",
       "AI agents are becoming increasingly sophisticated, with enhanced reasoning capabilities, multi-modal integration, open-source development, local processing, and cost efficiency [7]. Major technology companies, such as Microsoft, Google, and Amazon Web Services, have offered platforms for deploying pre-built AI agents [8]. Notable AI agents and platforms include OpenAI Operator, ChatGPT Agent, Devin AI, and Perplexity AI [9].\n",
       "\n",
       "### Financial Implications\n",
       "The cost of implementing and maintaining AI agents is a significant consideration for organizations. While estimates vary, the cost of developing and deploying AI agents is expected to decrease as the technology becomes more widespread [10]. However, the potential cost savings from increased efficiency and productivity are substantial, with some estimates suggesting that AI agents could save organizations millions of dollars in labor costs [11].\n",
       "\n",
       "### Risks\n",
       "The growing use of AI agents introduces new security risks, with researchers identifying a wave of attacks targeting AI agents' features [12]. To mitigate these risks, organizations must design systems that can detect, contain, and recover from misuse [13]. This includes implementing controls to limit how far agent-driven attacks can spread and how much damage they can cause [14].\n",
       "\n",
       "### Real-World Applications\n",
       "AI agents are being applied in various industries, such as e-commerce, global supply chain, and healthcare [15]. In e-commerce, AI agents are handling customer inquiries, managing inventory, and optimizing pricing [16]. In the global supply chain, AI agents are coordinating logistics, sourcing materials, and managing distribution [17]. In healthcare, AI agents are providing 24/7 support for coverage queries, eligibility questions, or claim statuses [18].\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "The future of AI agents in 2025 is poised to revolutionize numerous aspects of our lives, from healthcare and education to finance and transportation. As AI technology continues to advance at an unprecedented rate, we can expect to see more sophisticated and autonomous AI agents that can learn, adapt, and interact with humans in a more seamless and intuitive way. With the potential to drive significant economic growth, improve productivity, and enhance overall quality of life, the integration of AI agents into our daily lives is an exciting and inevitable development that will shape the world of tomorrow. As we embark on this new frontier, it is essential to prioritize responsible AI development, ensuring that these powerful technologies are harnessed for the betterment of society and humanity as a whole.\n",
       "\n",
       "## Sources\n",
       "1. https://www.salesforce.com/news/stories/future-of-ai-agents-2025/\n",
       "2. https://thenewstack.io/ai-engineering-trends-in-2025-agents-mcp-and-vibe-coding/\n",
       "3. https://unit42.paloaltonetworks.com/agentic-ai-threats/\n",
       "4. https://www.gartner.com/en/newsroom/press-releases/2025-08-26-gartner-predicts-40-percent-of-enterprise-apps-will-feature-task-specific-ai-agents-by-2026-up-from-less-than-5-percent-in-2025\n",
       "5. https://www.trendmicro.com/vinfo/us/security/news/threat-landscape/trend-micro-state-of-ai-security-report-1h-2025\n",
       "6. https://medium.com/aimonks/building-multi-agent-ai-systems-in-2025-the-no-code-revolution-democratizing-enterprise-ai-a0be590d5b10\n",
       "7. https://cloud.google.com/transform/ai-impact-industries-2025\n",
       "8. https://www.oneadvanced.com/resources/future-of-ai-agents/\n",
       "9. https://theriseofthedigitalworkforce.cio.com/theciosguidetoagenticai/thefutureofaiagents/\n",
       "10. https://www.salesforce.com/au/news/stories/the-future-of-ai-agents-top-predictions-trends-to-watch-in-2026/\n",
       "11. https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality\n",
       "12. https://www.esecurityplanet.com/artificial-intelligence/ai-agent-attacks-in-q4-2025-signal-new-risks-for-2026/\n",
       "13. https://www.linkedin.com/pulse/future-ai-agents-transforming-business-society-2025-beyond-john-enoh-hjo6c"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# CONFIGURATION\n",
    "max_analysts = 3 \n",
    "topic = \"The Future of AI Agents in 2025\"\n",
    "thread = {\"configurable\": {\"thread_id\": \"999\"}} # Changed ID to clear history\n",
    "\n",
    "# PHASE 1\n",
    "print(f\"üïµÔ∏è Generating Analysts...\")\n",
    "for event in graph.stream({\"topic\": topic, \"max_analysts\": max_analysts}, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        print(f\"‚úÖ Generated {len(analysts)} Analysts.\")\n",
    "        for a in analysts:\n",
    "            print(f\"   - {a.name} ({a.role})\")\n",
    "\n",
    "# PHASE 2\n",
    "print(\"\\nüöÄ Starting Parallel Research (This may take 2-3 minutes due to staggering)...\")\n",
    "graph.update_state(thread, {\"human_analyst_feedback\": None}, as_node=\"human_feedback\")\n",
    "\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(f\"-- Finished Node: {node_name}\")\n",
    "\n",
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"FINAL REPORT\")\n",
    "print(\"=\"*30)\n",
    "Markdown(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
